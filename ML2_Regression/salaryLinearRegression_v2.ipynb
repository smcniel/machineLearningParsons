{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sal = pd.read_csv('./data/mergedSalary2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eliminate rows with missing Y values (NaN)\n",
    "sal['missingSalary'] = pd.isnull(sal['salary'])\n",
    "sal2 = sal[(sal.missingSalary == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of X vars to include\n",
    "X_numeric_features = ['sup1', 'sup2', 'sup3', 'sup4', 'sup5', 'disabled', 'yearsinposition', \n",
    "                                    'yearsinprofession', 'age', 'cred1', 'cred2', 'inst1', 'inst2', 'inst3', 'inst4', \n",
    "                                    'inst5', 'instbudget', 'instsize', 'median_household_income', 'no_male_hs',\n",
    "                                    'no_female_hs', 'no_hs', 'at_least_hs_male', 'at_least_hs_female',\n",
    "                                    'at_least_hs', 'hs_some_college_male', 'hs_some_college_female',\n",
    "                                    'hs_some_college', 'bachelors_male', 'bachelors_female', 'bachelors',\n",
    "                                    'at_least_bach_male', 'at_least_bach_female', 'at_least_bach',\n",
    "                                    'graduate_male', 'graduate_female', 'graduate', 'hispanic', 'white',\n",
    "                                    'black', 'native_american', 'asian_api', 'two_race_or_more',\n",
    "                                    'asian_api_total', 'latino_total', 'white_total', 'native_american_total',\n",
    "                                    'two_race_or_more_total', 'male_unemployment', 'female_unemployment',\n",
    "                                    'renter', 'owner', 'median_rent', 'Sex.by.Age..Male.',\n",
    "                                    'Sex.by.Age..Male..Under.5.years', 'Sex.by.Age..Male..5.to.9.years',\n",
    "                                    'Sex.by.Age..Male..10.to.14.years', 'Sex.by.Age..Male..15.to.17.years',\n",
    "                                    'Sex.by.Age..Male..18.and.19.years', 'Sex.by.Age..Male..20.years',\n",
    "                                    'Sex.by.Age..Male..21.years', 'Sex.by.Age..Male..22.to.24.years',\n",
    "                                    'Sex.by.Age..Male..25.to.29.years', 'Sex.by.Age..Male..30.to.34.years',\n",
    "                                    'Sex.by.Age..Male..35.to.39.years', 'Sex.by.Age..Male..40.to.44.years',\n",
    "                                    'Sex.by.Age..Male..45.to.49.years', 'Sex.by.Age..Male..50.to.54.years',\n",
    "                                    'Sex.by.Age..Male..55.to.59.years', 'Sex.by.Age..Male..60.and.61.years',\n",
    "                                    'Sex.by.Age..Male..62.to.64.years', 'Sex.by.Age..Male..65.and.66.years',\n",
    "                                    'Sex.by.Age..Male..67.to.69.years', 'Sex.by.Age..Male..70.to.74.years',\n",
    "                                    'Sex.by.Age..Male..75.to.79.years', 'Sex.by.Age..Male..80.to.84.years',\n",
    "                                    'Sex.by.Age..Male..85.years.and.over', 'Sex.by.Age..Female.',\n",
    "                                    'Sex.by.Age..Female..Under.5.years', 'Sex.by.Age..Female..5.to.9.years',\n",
    "                                    'Sex.by.Age..Female..10.to.14.years', 'Sex.by.Age..Female..15.to.17.years', \n",
    "                                    'Sex.by.Age..Female..18.and.19.years', 'Sex.by.Age..Female..20.years',\n",
    "                                    'Sex.by.Age..Female..22.to.24.years', 'Sex.by.Age..Female..25.to.29.years',\n",
    "                                    'Sex.by.Age..Female..30.to.34.years', 'Sex.by.Age..Female..35.to.39.years',\n",
    "                                    'Sex.by.Age..Female..40.to.44.years', 'Sex.by.Age..Female..45.to.49.years',\n",
    "                                    'Sex.by.Age..Female..50.to.54.years', 'Sex.by.Age..Female..55.to.59.years',\n",
    "                                    'Sex.by.Age..Female..60.and.61.years', 'Sex.by.Age..Female..62.to.64.years',\n",
    "                                    'Sex.by.Age..Female..65.and.66.years', 'Sex.by.Age..Female..67.to.69.years',\n",
    "                                    'Sex.by.Age..Female..70.to.74.years', 'Sex.by.Age..Female..75.to.79.years',\n",
    "                                    'Sex.by.Age..Female..80.to.84.years', 'Sex.by.Age..Female..85.years.and.over',\n",
    "                                    'full_time', 'part_time', 'foreign_born', 'US_born', 'married', 'divorced',\n",
    "                                    'poverty']\n",
    "X_numeric = sal2[X_numeric_features]\n",
    "X_categorical_features = ['function', 'gender', 'race', 'highestdegree', 'category', 'insttype']\n",
    "X_categorical = sal2[X_categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dummy variables for each of the categorical features\n",
    "# DOC: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\n",
    "\n",
    "function_dummies = pd.get_dummies(X_categorical['function'])\n",
    "gender_dummies = pd.get_dummies(X_categorical['gender'])\n",
    "race_dummies = pd.get_dummies(X_categorical['race'])\n",
    "highestDegree_dummies = pd.get_dummies(X_categorical['highestdegree'])\n",
    "category_dummies = pd.get_dummies(X_categorical['category'])\n",
    "instType_dummies = pd.get_dummies(X_categorical['insttype'])\n",
    "\n",
    "# convert to ndarray\n",
    "X_dummy_features = pd.concat([function_dummies, gender_dummies, race_dummies, highestDegree_dummies, category_dummies, instType_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing values in numerical features\n",
    "# DOC: http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer()\n",
    "imp.fit(X_numeric)\n",
    "X_numeric_imputed = imp.transform(X_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((X_dummy_features, X_numeric_imputed), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sal2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y is salary\n",
    "y = sal2.iloc[:, 8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep track of variance on test data, to graph\n",
    "var_to_graph = {}\n",
    "# bring residual sum of squares from regression1.ipynb\n",
    "var_to_graph['simpReg'] = 265376883.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training and test sets for linear regression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import models for Ridge, Lasso, Linear Regression\n",
    "from sklearn import datasets, linear_model\n",
    "# import for polynomial fitting\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "X_train_no_intercept = X_train\n",
    "X_train = X_train.reshape(-1, X_train.shape[1])\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Create residual plot to check for non random patterns in errors\n",
    "# This checks for things like multicollinearity\n",
    "y_train_pred = regr.predict(X_train)\n",
    "y_test_pred = regr.predict(X_test)\n",
    "\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Training data')\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, c='lightgreen', marker='s', label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='uppper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red')\n",
    "plt.xlim([-10, 50])\n",
    "plt.show()\n",
    "\n",
    "# The intercept\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean square error\n",
    "print(\"Residual sum of squares, training data: %.2f\"\n",
    "      % np.mean((regr.predict(X_train) - y_train) ** 2))\n",
    "print(\"Residual sum of squares, test data: %.2f\"\n",
    "      % np.mean((regr.predict(X_test) - y_test) ** 2))\n",
    "var_to_graph['multReg_linear'] = np.mean((regr.predict(X_test) - y_test) ** 2)\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score, training data: %.2f' % regr.score(X_train, y_train))\n",
    "\n",
    "# More accurate variance score generated from the adjusted R-squared\n",
    "# Every time a predictor is added to the model, the R-squared increases no matter what\n",
    "# Adjusted R-squared takes into account number of predictor variables P\n",
    "# and number of observations N\n",
    "# to do: add adj score\n",
    "\n",
    "#vector of prediction error\n",
    "print('Distribution of prediction error on training data:')\n",
    "predError = regr.predict(X_train) - y_train\n",
    "plt.hist(predError)\n",
    "plt.show()\n",
    "\n",
    "print('Distribution of prediction error on test data:')\n",
    "predError = regr.predict(X_test) - y_test\n",
    "plt.hist(predError)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training and test sets for polynomial linear regression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_poly, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## POLYNOMINAL \n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Create linear regression object\n",
    "poly = linear_model.LinearRegression(normalize=True)\n",
    "\n",
    "# Train the model using the training sets\n",
    "X_train_no_intercept = X_train\n",
    "X_train = X_train.reshape(-1, X_train.shape[1])\n",
    "poly.fit(X_train, y_train)\n",
    "\n",
    "# Create residual plot \n",
    "y_train_pred = poly.predict(X_train)\n",
    "y_test_pred = poly.predict(X_test)\n",
    "\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Training data')\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, c='lightgreen', marker='s', label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='uppper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red')\n",
    "plt.xlim([-10, 50])\n",
    "plt.show()\n",
    "\n",
    "# The intercept\n",
    "print('Intercept: \\n', poly.intercept_)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', poly.coef_)\n",
    "# The mean square error\n",
    "print(\"Residual sum of squares, training data: %.2f\"\n",
    "      % np.mean((poly.predict(X_train) - y_train) ** 2))\n",
    "print(\"Residual sum of squares, test data: %.2f\"\n",
    "      % np.mean((poly.predict(X_test) - y_test) ** 2))\n",
    "var_to_graph['multReg_poly'] = np.mean((poly.predict(X_test) - y_test) ** 2)\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score, training data: %.2f' % poly.score(X_train, y_train))\n",
    "#vector of prediction error\n",
    "print('Distribution of prediction error on training data:')\n",
    "predError = poly.predict(X_train) - y_train\n",
    "plt.hist(predError)\n",
    "plt.show()\n",
    "\n",
    "print('Distribution of prediction error on test data:')\n",
    "predError = poly.predict(X_test) - y_test\n",
    "plt.hist(predError)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training and test sets for Ridge regression\n",
    "# Ridge adds additional regularization\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## RIDGE REGRESSION\n",
    "# DOC: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "# Create linear regression object\n",
    "ridge_regr = linear_model.Ridge()\n",
    "\n",
    "# Train the model using the training sets\n",
    "X_train_no_intercept = X_train\n",
    "X_train = X_train.reshape(-1, X_train.shape[1])\n",
    "ridge_regr.fit(X_train, y_train)\n",
    "\n",
    "# Create residual plot \n",
    "y_train_pred = ridge_regr.predict(X_train)\n",
    "y_test_pred = ridge_regr.predict(X_test)\n",
    "\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Training data')\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, c='lightgreen', marker='s', label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='uppper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red')\n",
    "plt.xlim([-10, 50])\n",
    "plt.show()\n",
    "\n",
    "# The intercept\n",
    "print('Intercept: \\n', ridge_regr.intercept_)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', ridge_regr.coef_)\n",
    "# The mean square error\n",
    "print(\"Residual sum of squares, training data: %.2f\"\n",
    "      % np.mean((ridge_regr.predict(X_train) - y_train) ** 2))\n",
    "print(\"Residual sum of squares, test data: %.2f\"\n",
    "      % np.mean((ridge_regr.predict(X_test) - y_test) ** 2))\n",
    "var_to_graph['multReg_ridge'] = np.mean((ridge_regr.predict(X_test) - y_test) ** 2)\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score, training data: %.2f' % ridge_regr.score(X_train, y_train))\n",
    "#vector of prediction error\n",
    "print('Distribution of prediction error on training data:')\n",
    "predError = ridge_regr.predict(X_train) - y_train\n",
    "plt.hist(predError)\n",
    "plt.show()\n",
    "\n",
    "print('Distribution of prediction error on test data:')\n",
    "predError = ridge_regr.predict(X_test) - y_test\n",
    "plt.hist(predError)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training and test sets for Lasso model\n",
    "# Ridge adds additional regularization\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## LASSO\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "las = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "# Train the model using the training sets\n",
    "X_train_no_intercept = X_train\n",
    "X_train = X_train.reshape(-1, X_train.shape[1])\n",
    "las.fit(X_train, y_train)\n",
    "\n",
    "# Create residual plot \n",
    "y_train_pred = las.predict(X_train)\n",
    "y_test_pred = las.predict(X_test)\n",
    "\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Training data')\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, c='lightgreen', marker='s', label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='uppper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red')\n",
    "plt.xlim([-10, 50])\n",
    "plt.show()\n",
    "\n",
    "# The intercept\n",
    "print('Intercept: \\n', las.intercept_)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', las.coef_)\n",
    "# The mean square error\n",
    "print(\"Residual sum of squares, training data: %.2f\"\n",
    "      % np.mean((las.predict(X_train) - y_train) ** 2))\n",
    "print(\"Residual sum of squares, test data: %.2f\"\n",
    "      % np.mean((las.predict(X_test) - y_test) ** 2))\n",
    "var_to_graph['multReg_ridge'] = np.mean((las.predict(X_test) - y_test) ** 2)\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score, training data: %.2f' % las.score(X_train, y_train))\n",
    "#vector of prediction error\n",
    "print('Distribution of prediction error on training data:')\n",
    "predError = las.predict(X_train) - y_train\n",
    "plt.hist(predError)\n",
    "plt.show()\n",
    "\n",
    "print('Distribution of prediction error on test data:')\n",
    "predError = las.predict(X_test) - y_test\n",
    "plt.hist(predError)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
